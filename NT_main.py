import os
import sys
from dataclasses import dataclass
from typing import Any, Literal

import numpy as np
import pandas as pd
import torch
from sklearn.base import BaseEstimator
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
from tqdm.auto import tqdm
from transformers import AutoModel, AutoTokenizer

# å‹æ…‹
DownstreamModel = Literal["xgboost", "random_forest", "logistic_regression"]


# é…ç½®
@dataclass
class Config:
    # è·¯å¾‘è¨­å®š
    DATASET_DIR: str = "dataset"
    EMB_DIR: str = "embeddings"
    TRAIN_FILE: str = "train"
    TEST_FILE: str = "test"

    # embedding æ¨¡å‹è¨­å®š
    MODEL_NAME: str = "InstaDeepAI/nucleotide-transformer-500m-human-ref"
    MAX_LENGTH: int = 1000  # æ¨¡å‹æœ¬èº«é™åˆ¶
    BATCH_SIZE: int = 400  # for RTX 4070

    # device è¨­å®š
    DEVICE: str = "cuda" if torch.cuda.is_available() else "cpu"


# DNA -> embeddings
class NucleotideFeatureExtractor:
    def __init__(self, model_name: str, device: str, max_length: int, batch_size: int) -> None:
        self.device = device
        self.max_length = max_length
        self.batch_size = batch_size

        print(f"Loading model: {model_name} on {self.device}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModel.from_pretrained(
            model_name,
            trust_remote_code=True,
            use_safetensors=True,
            dtype=torch.float16,  # é€Ÿåº¦è¼ƒå¿«
        )
        self.model.to(self.device)
        self.model.eval()

    def extract(self, sequences: list[str]) -> np.ndarray:
        embeddings_list: list[np.ndarray] = []

        for i in tqdm(range(0, len(sequences), self.batch_size), desc="Extracting Embeddings"):
            batch_seqs = sequences[i : i + self.batch_size]

            # Tokenization
            inputs = self.tokenizer(
                batch_seqs,
                return_tensors="pt",
                padding=True,  # batch æ¯ç­†è³‡æ–™ç”¢å‡º token æ•¸é‡å°é½Š (attention_mask ç´€éŒ„ padding ä½ç½®ï¼Œæ˜¯ç‚º 0 ä¸æ˜¯ç‚º 1)
                truncation=True,  # ç•¶ batch çš„ä¸€ç­†è³‡æ–™ç”¢å‡ºçš„ token æ•¸é‡è¶…é max_lengthï¼Œæˆªæ–·å®ƒ
                max_length=self.max_length,
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # Inference
            with torch.no_grad():
                outputs = self.model(**inputs)
                hidden_states = outputs.last_hidden_state  # (B, L, D)
                attention_mask = inputs["attention_mask"].unsqueeze(-1)  # (B, L, 1)

                # mean pooling (æ’é™¤ padding çš„å½±éŸ¿)
                sum_embeddings = torch.sum(hidden_states * attention_mask, dim=1)  # (B, D)
                sum_mask = torch.clamp(attention_mask.sum(dim=1), min=1e-9)  # (B, 1)
                mean_embeddings = sum_embeddings / sum_mask  # (B, D)

                embeddings_list.append(mean_embeddings.cpu().numpy())

        return np.vstack(embeddings_list)


# è³‡æ–™è™•ç†
class SequenceDataProcessor:
    def __init__(self, config: Config) -> None:
        self.config = config
        self.label_encoder = LabelEncoder()

    @staticmethod
    def _clean_sequence(sequence: str | float) -> str:
        if not isinstance(sequence, str):
            return ""
        return sequence.replace("<", "").replace(">", "").strip()

    def _get_embeddings(self, emb_file_path: str, sequences: list[str] | None = None) -> np.ndarray:
        if os.path.exists(emb_file_path):
            print(f"Found saved embeddings. Loading from {emb_file_path}...")
            return np.load(emb_file_path)

        if sequences is None:
            raise ValueError(f"Cache not found at {emb_file_path} and no sequences provided for extraction.")

        print(f"No saved embeddings found at {emb_file_path}. Starting extraction...")

        # extractor lazy loading
        if self.extractor is None:
            self.extractor = NucleotideFeatureExtractor(
                model_name=self.config.MODEL_NAME,
                device=self.config.DEVICE,
                max_length=self.config.MAX_LENGTH,
                batch_size=self.config.BATCH_SIZE,
            )

        embeddings = self.extractor.extract(sequences)

        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(emb_file_path), exist_ok=True)
        np.save(emb_file_path, embeddings)
        print(f"Saved embeddings to {emb_file_path}")

        return embeddings

    def load_data(self) -> tuple[tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray], LabelEncoder]:
        print("Loading CSV files...")

        # åŸæœ¬ dataset
        df_train = pd.read_csv(os.path.join(self.config.DATASET_DIR, f"{self.config.TRAIN_FILE}.csv"))
        df_test = pd.read_csv(os.path.join(self.config.DATASET_DIR, f"{self.config.TEST_FILE}.csv"))

        # è™•ç†å¾Œçš„ embeddings (ä½œç‚º features çµ¦ä¸‹æ¸¸æ¨¡å‹ä½¿ç”¨)
        train_emb_path = os.path.join(self.config.EMB_DIR, f"{self.config.TRAIN_FILE}.npy")
        test_emb_path = os.path.join(self.config.EMB_DIR, f"{self.config.TEST_FILE}.npy")

        # è‹¥ embeddings æª”æ¡ˆä¸å­˜åœ¨æ™‚ï¼ŒåŸ·è¡Œ clean_sequence (æº–å‚™æ‹¿ä¾†è¨ˆç®— embeddings)
        train_seqs = None
        if not os.path.exists(train_emb_path):
            print("Processing train sequences for extraction...")
            train_seqs = df_train["NucleotideSequence"].apply(self._clean_sequence).tolist()

        test_seqs = None
        if not os.path.exists(test_emb_path):
            print("Processing test sequences for extraction...")
            test_seqs = df_test["NucleotideSequence"].apply(self._clean_sequence).tolist()

        # è‹¥ embeddings æª”æ¡ˆä¸å­˜åœ¨æ™‚ï¼Œè¨ˆç®— embeddings (å¦å‰‡ç›´æ¥è®€å–æª”æ¡ˆ)
        X_train = self._get_embeddings(train_emb_path, sequences=train_seqs)
        X_test = self._get_embeddings(test_emb_path, sequences=test_seqs)

        # æº–å‚™ labels
        print("Encoding labels...")
        y_train = self.label_encoder.fit_transform(df_train["GeneType"])
        y_test = self.label_encoder.transform(df_test["GeneType"])

        print(f"Classes: {self.label_encoder.classes_}")
        print(f"Final Feature Shapes -> Train: {X_train.shape}, Test: {X_test.shape}")

        return (X_train, X_test, y_train, y_test), self.label_encoder


# æ¨¡å‹å·¥å» 
class ModelFactory:
    @staticmethod
    def get_model(name: DownstreamModel, **kwargs: Any) -> tuple[BaseEstimator, dict[str, Any]]:
        name = name.lower()

        if name == "logistic_regression":
            return ModelFactory._get_logistic_regression()
        elif name == "random_forest":
            return ModelFactory._get_random_forest()
        elif name == "xgboost":
            if "data" not in kwargs:
                raise ValueError("XGBoost requires 'data' in kwargs for eval_set.")
            return ModelFactory._get_xgboost(kwargs["data"])
        else:
            raise ValueError(f"Unknown model name: {name}")

    @staticmethod
    def _get_logistic_regression() -> tuple[BaseEstimator, dict[str, Any]]:
        from sklearn.linear_model import LogisticRegression

        model = LogisticRegression(max_iter=2000, class_weight="balanced", C=1.0)
        return model, {}

    @staticmethod
    def _get_random_forest() -> tuple[BaseEstimator, dict[str, Any]]:
        from sklearn.ensemble import RandomForestClassifier

        model = RandomForestClassifier(n_estimators=200, class_weight="balanced", n_jobs=-1)
        return model, {}

    @staticmethod
    def _get_xgboost(
        data: tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],
    ) -> tuple[BaseEstimator, dict[str, Any]]:
        from xgboost import XGBClassifier

        X_train, X_test, y_train, y_test = data

        model = XGBClassifier(
            n_estimators=500,
            learning_rate=0.05,
            max_depth=6,
            tree_method="hist",
            device=Config.DEVICE,
            eval_metric="mlogloss",
            early_stopping_rounds=20,
        )

        fit_params = {"eval_set": [(X_test, y_test)], "verbose": False}
        return model, fit_params


# å¯¦é©—
def run_experiment(
    model: BaseEstimator,
    data: tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],
    label_encoder: LabelEncoder,
    exp: str,
    **fit_kwargs: Any,
) -> None:
    X_train, X_test, y_train, y_test = data
    print(f"\n{'=' * 10} [{exp}] {'=' * 10}")

    # è¨“ç·´
    try:
        model.fit(X_train, y_train, **fit_kwargs)
    except Exception as e:
        print(f"âŒ Training failed: {e}")
        return

    # è©•ä¼°
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"ğŸ† Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))


# ä¸»ç¨‹å¼
def main() -> None:
    # 1. æº–å‚™è³‡æ–™
    try:
        # åˆä½µ load_dataset + feature extraction æµç¨‹
        processor = SequenceDataProcessor(config=Config())
        data, label_encoder = processor.load_data()
        print("âœ… Data Loaded and Processed Successfully.")
    except Exception as e:
        print(f"âŒ Error loading data: {e}")
        sys.exit(1)

    # 2. å¯¦é©—æ¸…å–®
    experiments: DownstreamModel = [
        # "logistic_regression",
        "random_forest",
        # "xgboost",
    ]

    # 3. è‡ªå‹•åŸ·è¡Œæ¸…å–®ä¸­çš„æ‰€æœ‰å¯¦é©—
    for exp in experiments:
        try:
            # å–å¾—æ¨¡å‹
            model, fit_params = ModelFactory.get_model(exp, data=data)
            # å°‡ embededding ä¸Ÿå…¥æ¨¡å‹
            run_experiment(model, data, label_encoder, exp, **fit_params)
        except ImportError as e:
            print(f"âš ï¸ Skip {exp}: {e}")
        except Exception as e:
            print(f"âŒ Error in {exp}: {e}")


if __name__ == "__main__":
    main()
